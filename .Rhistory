quantity_Y_4000 <- 160 - 40 * price  # Demand when income = $4000
quantity_Y_5000 <- 170 - 40 * price  # Demand when income = $5000
# Combine data into a data frame
data <- data.frame(
Price = rep(price, 2),
Quantity = c(quantity_Y_4000, quantity_Y_5000),
Income = rep(c("$4000", "$5000"), each = length(price))
)
# Plot the demand curves with Quantity on x-axis and Price on y-axis
ggplot(data, aes(x = Quantity, y = Price, color = Income)) +
geom_line(size = 1) +
labs(
title = "Shift in Demand Curve for Avocados as Income Increases",
x = "Quantity Demanded (Q)",
y = "Price of Avocados ($)",
color = "Per Capita Income"
) +
theme_minimal() +
annotate(
"text", x = 140, y = 2.5,
label = "Demand shifts outward\nwith higher income",
color = "black", size = 4, hjust = 0
)
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
crime_data93 = af_crime93
# This is the main variable we are trying to predict using the other features.
dim(crime_data93)
# There are 51 observations and 8 variables
# One of which is the target variable = violent
set.seed(123)
trainIndex <- createDataPartition(af_crime93$violent, p = 0.8, list = FALSE)
train_data <- af_crime93[trainIndex, ]
test_data <- af_crime93[-trainIndex, ]
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
train_data_norm <- predict(preProc, train_data[, -1])
test_data_norm <- predict(preProc, test_data[, -1])
setwd("C:/Users/fren/Desktop/Assignments for AI & Machine Learning/Assignment-3")
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
crime_data93 = af_crime93
# This is the main variable we are trying to predict using the other features.
dim(crime_data93)
# There are 51 observations and 8 variables
# One of which is the target variable = violent
set.seed(123)
trainIndex <- createDataPartition(af_crime93$violent, p = 0.8, list = FALSE)
train_data <- af_crime93[trainIndex, ]
test_data <- af_crime93[-trainIndex, ]
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
set.seed(123)
# Split the data into 80% training and 20% testing
data_split = initial_split(af_crime93, prop = 0.8)
train_data = training(data_split)
test_data = testing(data_split)
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
crime_data93 = af_crime93
library("tidyverse")
install.packages("stevedata")
library("stevedata")
install.packages("tidytuesdayR")
library("tidytuesdayR")
library(tidymodels)
install.packages("caret")
library(caret)
crime_data93 = af_crime93
# This is the main variable we are trying to predict using the other features.
dim(crime_data93)
# There are 51 observations and 8 variables
# One of which is the target variable = violent
set.seed(123)
# Split the data into 80% training and 20% testing
data_split = initial_split(af_crime93, prop = 0.8)
train_data = training(data_split)
test_data = testing(data_split)
View(data_split)
View(test_data)
View(train_data)
View(test_data)
View(crime_data93)
# Normalize the data (excluding the first column, which is likely categorical)
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
# Apply normalization to training and testing datasets
train_data_norm <- predict(preProc, train_data[, -1])
test_data_norm <- predict(preProc, test_data[, -1])
View(preProc)
View(test_data_norm)
View(train_data_norm)
View(crime_data93)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
# Train a basic linear regression model
lm_model <- train(violent ~ ., data = train_data, method = "lm")
summary(lm_model)
View(lm_model)
# Train Lasso with best lambda
final_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
# Load necessary package
library(glmnet)
# Define predictors and target variable
x_train <- as.matrix(train_data[, -c(1,2)])  # Remove 'state' and 'violent' columns
y_train <- train_data$violent
# Set up lambda values to test
lambda_seq <- 10^seq(-3, 3, length = 100)  # Testing from 0.001 to 1000
# Train Lasso regression with cross-validation
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_seq)
# Find best lambda
best_lambda <- lasso_model$lambda.min
print(best_lambda)
# Train Lasso with best lambda
final_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
# Train Ridge (if preferred)
final_ridge <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda)
View(final_lasso)
# Load necessary libraries
library(tidymodels)
library(caret)
# Set seed for reproducibility
set.seed(123)
# Step 1: Split the Data into Training and Testing Sets
data_split <- initial_split(af_crime93, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 2: Normalize the Data (Excluding the First Column, Assuming Itâ€™s Categorical)
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
# Apply Normalization to Training and Testing Datasets
train_data_norm <- predict(preProc, train_data[, -1])
test_data_norm <- predict(preProc, test_data[, -1])
# Step 3: Define and Train the Linear Regression Model
lm_spec <- linear_reg() %>%
set_engine("lm")
# Fit the Model Using Training Data
lm_fit <- lm_spec %>%
fit(violent ~ ., data = train_data)
# Step 4: Make Predictions on Test Data
predictions <- predict(lm_fit, test_data) %>%
bind_cols(test_data)
# Load necessary libraries
library(tidymodels)
library(caret)
# Set seed for reproducibility
set.seed(123)
# Step 1: Split the Data into Training and Testing Sets
data_split <- initial_split(af_crime93, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
# Ensure test_data has the same factor levels as train_data for categorical variables
train_data$state <- as.factor(train_data$state)
test_data$state <- factor(test_data$state, levels = levels(train_data$state))
# Step 2: Normalize the Data (Excluding Categorical Variables)
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
# Apply Normalization to Training and Testing Datasets (excluding categorical variables)
train_data_norm <- predict(preProc, train_data[, -1])
test_data_norm <- predict(preProc, test_data[, -1])
# Step 3: Define and Train the Linear Regression Model
lm_spec <- linear_reg() %>%
set_engine("lm")
# Fit the Model Using Training Data
lm_fit <- lm_spec %>%
fit(violent ~ ., data = train_data)
# Step 4: Make Predictions on Test Data (Fixed Factor Levels)
predictions <- predict(lm_fit, new_data = test_data) %>%
bind_cols(test_data)
# Step 5: Make Predictions on Training Data
train_predictions <- predict(lm_fit, new_data = train_data) %>%
bind_cols(train_data)
# Step 6: Visualizing Actual vs Predicted Values
library(ggplot2)
ggplot() +
geom_point(data = train_predictions, aes(x = police, y = violent), color = "darkgreen", alpha = 0.5) +
geom_line(data = train_predictions, aes(x = police, y = .pred), color = "darkgreen", size = 1, alpha = 0.3) +
geom_segment(data = train_predictions, aes(x = police, y = .pred, yend = violent), color = "green", size = 1, alpha = 0.3) +
geom_smooth(data = predictions, aes(x = police, y = violent), color = "blue", size = 1, alpha = 0.3, method = "lm") +
geom_point(data = predictions, aes(x = police, y = violent), color = "blue", alpha = 0.5) +
labs(title = "Actual vs Predicted Violent Crime Rate",
x = "Police Presence",
y = "Violent Crime Rate")
View(predictions)
# Load necessary libraries
library(tidymodels)
library(caret)
library(ggplot2)
# Set seed for reproducibility
set.seed(123)
# Step 1: Split the Data into Training and Testing Sets
data_split <- initial_split(af_crime93, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 2: Ensure test_data has the same factor levels as train_data
train_data$state <- as.factor(train_data$state)
test_data$state <- factor(test_data$state, levels = levels(train_data$state))
# Step 3: Check for Multicollinearity (Fixing Rank Deficiency)
cor_matrix <- cor(train_data %>% select(-violent, -state))  # Exclude target variable
high_corr <- findCorrelation(cor_matrix, cutoff = 0.9)  # Identify highly correlated predictors
train_data <- train_data %>% select(-all_of(high_corr))  # Drop correlated variables
test_data <- test_data %>% select(-all_of(high_corr))  # Keep test data consistent
# Step 4: Normalize the Data (Excluding Categorical Variables)
preProc <- preProcess(train_data %>% select(-state), method = c("center", "scale"))
# Apply Normalization to Training and Testing Datasets (excluding categorical variables)
train_data_norm <- predict(preProc, train_data %>% select(-state))
test_data_norm <- predict(preProc, test_data %>% select(-state))
# Add back the state column
train_data_norm$state <- train_data$state
test_data_norm$state <- test_data$state
# Step 5: Define and Train the Linear Regression Model
lm_spec <- linear_reg() %>%
set_engine("lm")
# Fit the Model Using Training Data
lm_fit <- lm_spec %>%
fit(violent ~ ., data = train_data_norm)
# Step 6: Make Predictions on Test Data
predictions <- predict(lm_fit, new_data = test_data_norm) %>%
bind_cols(test_data_norm)
# Step 7: Make Predictions on Training Data
train_predictions <- predict(lm_fit, new_data = train_data_norm) %>%
bind_cols(train_data_norm)
# Step 8: Visualizing Actual vs Predicted Values
ggplot() +
geom_point(data = train_predictions, aes(x = police, y = violent), color = "darkgreen", alpha = 0.5) +
geom_line(data = train_predictions, aes(x = police, y = .pred), color = "darkgreen", linewidth = 1, alpha = 0.3) +
geom_segment(data = train_predictions, aes(x = police, y = .pred, yend = violent), color = "green", linewidth = 1, alpha = 0.3) +
geom_smooth(data = predictions, aes(x = police, y = violent), color = "blue", linewidth = 1, alpha = 0.3, method = "lm") +
geom_point(data = predictions, aes(x = police, y = violent), color = "blue", alpha = 0.5) +
labs(title = "Actual vs Predicted Violent Crime Rate",
x = "Police Presence",
y = "Violent Crime Rate")
crime_data93 = af_crime93
dim(crime_data93)
# There are 51 observations and 8 variables
# One of which is the target variable = violent
library(tidymodels)
set.seed(123)
# Define Ridge Regression Model
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet")
# Create Cross-validation Folds
cv_folds <- vfold_cv(train_data, v = 5)
set.seed(123)
# Split the data into 80% training and 20% testing
data_split = initial_split(af_crime93, prop = 0.8)
train_data = training(data_split)
test_data = testing(data_split)
library(tidymodels)
set.seed(123)
# Define Ridge Regression Model
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet")
# Create Cross-validation Folds
cv_folds <- vfold_cv(train_data, v = 5)
# Set up Grid of Lambda Values to Try
lambda_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 100)
# Define Recipe
ridge_recipe <- recipe(violent ~ ., data = train_data)
# Create Workflow
ridge_wf <- workflow() %>%
add_model(ridge_spec) %>%
add_recipe(ridge_recipe)
# Tune the Model
tune_results <- tune_grid(
ridge_wf,
resamples = cv_folds,
grid = lambda_grid
)
# Find Best Lambda
best_lambda <- select_best(tune_results, metric = "rmse")
library(tidymodels)
set.seed(123)
# Define Ridge Regression Model
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet")
# Create Cross-validation Folds
cv_folds <- vfold_cv(train_data, v = 5)
ridge_recipe <- recipe(sale.price ~ gross.square.feet + state, data = train) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%  # Convert categorical variables to numeric
step_normalize(all_numeric_predictors())  # Normalize numeric predictors
# Set up Grid of Lambda Values to Try
lambda_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 100)
# Define Recipe
ridge_recipe <- recipe(violent ~ ., data = train_data)
# Create Workflow
ridge_wf <- workflow() %>%
add_model(ridge_spec) %>%
add_recipe(ridge_recipe)
# Tune the Model
tune_results <- tune_grid(
ridge_wf,
resamples = cv_folds,
grid = lambda_grid
)
# Find Best Lambda
best_lambda <- select_best(tune_results, metric = "rmse")
library(tidymodels)
set.seed(123)
# Define Ridge Regression Model
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet")
# Create Cross-validation Folds
cv_folds <- vfold_cv(train_data, v = 5)
# Define Recipe (Ensure Correct Response Variable)
ridge_recipe <- recipe(sale.price ~ gross.square.feet + state, data = train_data) %>%
step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%  # Convert categorical variables to numeric
step_normalize(all_numeric_predictors())  # Normalize numeric predictors
crime_data93 = af_crime93
summary(crime_93)
summary(crime_data93)
str(crime_data93)
set.seed(123)
# Split the data into 80% training and 20% testing
data_split = initial_split(af_crime93, prop = 0.8)
train_data = training(data_split)
test_data = testing(data_split)
# Normalize the data (excluding the first column, which is likely categorical)
preProc <- preProcess(train_data[, -1], method = c("center", "scale"))
# Apply normalization to training and testing datasets
train_data_norm <- predict(preProc, train_data[, -1])
test_data_norm <- predict(preProc, test_data[, -1])
set.seed(123)
lm_spec <- linear_reg() %>%
set_engine("lm")
crime_data93 = af_crime93
set.seed(123)
# Split the data into 80% training and 20% testing
data_split = initial_split(af_crime93, prop = 0.8)
train_data = training(data_split)
test_data = testing(data_split)
View(test_data)
View(train_data)
crime_model <- lm(violent ~ poverty + single + metro + white + highschool, data = train_data)
summary(crime_model)
library(car)
vif(crime_model)
install.packages("car")
library(car)
vif(crime_model)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
install.packages("car")
library(car)
vif(crime_model)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
install.packages("car")
library(car)
predictions <- predict(crime_model, test_data)
actual_values <- test_data$violent
rmse <- sqrt(mean((predictions - actual_values)^2))
print(rmse)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
install.packages("car")
library(car)
lm_spec <- linear_reg() %>%
set_engine("lm")
lm_fit <- lm_spec %>%
fit(violent ~ poverty + single + metro + white + highschool, data = train_data)
lm_fit
print(lm_fit)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
install.packages("car")
library(car)
test_predictions <- predict(lm_fit, new_data = test_data) %>%
bind_cols(test_data)
metrics <- yardstick::metrics(test_predictions, truth = violent, estimate = .pred)
print(metrics)
# Set CRAN mirror first
options(repos = c(CRAN = "https://cloud.r-project.org"))
# Load necessary libraries
library(tidyverse)
if (!requireNamespace("stevedata", quietly = TRUE)) {
install.packages("stevedata")
}
library(stevedata)
if (!requireNamespace("tidytuesdayR", quietly = TRUE)) {
install.packages("tidytuesdayR")
}
library(tidytuesdayR)
if (!requireNamespace("caret", quietly = TRUE)) {
install.packages("caret")
}
library(caret)
library(tidymodels)
library(glmnet)
install.packages("car")
library(car)
ggplot(test_predictions, aes(x = .pred, y = violent - .pred)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
